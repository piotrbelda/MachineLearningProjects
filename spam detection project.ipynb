{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_address=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_load=tf.keras.utils.get_file(url_address.split('/')[-1],url_address,extract=True,cache_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(os.getcwd(),\"datasets\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Piotrek\\\\TF2.0\\\\datasets'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['readme', 'SMSSpamCollection', 'smsspamcollection.zip']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SMSSpamCollection','r') as file:\n",
    "    head=[next(file) for x in range(10)]\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\\n', 'ham\\tOk lar... Joking wif u oni...\\n', \"spam\\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\n\", 'ham\\tU dun say so early hor... U c already then say...\\n', \"ham\\tNah I don't think he goes to usf, he lives around here though\\n\", \"spam\\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, Â£1.50 to rcv\\n\", 'ham\\tEven my brother is not like to speak with me. They treat me like aids patent.\\n', \"ham\\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\\n\", 'spam\\tWINNER!! As a valued network customer you have been selected to receivea Â£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\\n', 'spam\\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\\n']\n"
     ]
    }
   ],
   "source": [
    "print(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab0,text0=head[0].strip().split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label is : ham, content is : Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n"
     ]
    }
   ],
   "source": [
    "print(f\"label is : {lab0}, content is : {text0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels,content=[],[]\n",
    "with open('SMSSpamCollection','r') as file:\n",
    "    for line in file:\n",
    "        label,text=line.strip().split('\\t')\n",
    "        labels.append(1 if label=='spam' else 0)\n",
    "        content.append(text)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat... \n",
      "\n",
      "0 Ok lar... Joking wif u oni... \n",
      "\n",
      "1 Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x,y in list(zip(labels,content))[:3]:\n",
    "    print(x,y,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5574"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tokens for each unique word in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=tf.keras.preprocessing.text.Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting text into a numerical representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sequences=tokenizer.texts_to_sequences(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[49, 472, 4436, 843, 756, 659, 64, 8, 1328, 87, 123, 352, 1329, 148, 2996, 1330, 67, 58, 4437, 144], [46, 337, 1500, 473, 6, 1941], [47, 490, 8, 19, 4, 798, 902, 2, 176, 1942, 1106, 660, 1943, 2331, 261, 2332, 71, 1942, 2, 1944, 2, 338, 490, 556, 961, 73, 392, 174, 661, 393, 2997], [6, 248, 150, 23, 383, 2998, 6, 139, 154, 57, 150]]\n"
     ]
    }
   ],
   "source": [
    "print(text_sequences[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sequences_padding=tf.keras.preprocessing.sequence.pad_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0   49  472 4436  843  756  659   64    8 1328   87  123  352 1329\n",
      "   148 2996 1330   67   58 4437  144]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0   46  337 1500  473    6 1941]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0   47  490    8   19    4  798  902    2  176 1942\n",
      "  1106  660 1943 2331  261 2332   71 1942    2 1944    2  338  490  556\n",
      "   961   73  392  174  661  393 2997]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    6  248  150   23\n",
      "   383 2998    6  139  154   57  150]]\n"
     ]
    }
   ],
   "source": [
    "print(text_sequences_padding[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating labels; it is a classification task, so there are 2 different possible outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_labels=tf.keras.utils.to_categorical(labels,num_classes=len(set(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating inverse dictionary from relation: word->index to index->word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 1), ('to', 2), ('you', 3), ('a', 4), ('the', 5), ('u', 6), ('and', 7), ('in', 8), ('is', 9), ('me', 10)]\n"
     ]
    }
   ],
   "source": [
    "word2idx=tokenizer.word_index\n",
    "print(list(word2idx.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word={v:k for k,v in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx['PAD']=0\n",
    "idx2word[0]='PAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9013"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size=len(word2idx)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=tf.data.Dataset.from_tensor_slices((text_sequences_padding,cat_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.shuffle(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into train/test/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5574"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_records=len(text_sequences_padding)\n",
    "num_of_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size=num_of_records // 4\n",
    "val_size=(num_of_records-test_size) // 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset=dataset.take(test_size)\n",
    "val_dataset=dataset.skip(test_size).take(val_size)\n",
    "train_dataset=dataset.skip(test_size+val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "test_dataset=test_dataset.batch(batch_size,drop_remainder=True)\n",
    "val_dataset=val_dataset.batch(batch_size,drop_remainder=True)\n",
    "train_dataset=train_dataset.batch(batch_size,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_records': 400000,\n",
       " 'file_size': 394362229,\n",
       " 'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
       " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-300/__init__.py',\n",
       " 'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       " 'parameters': {'dimension': 300},\n",
       " 'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       " 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-300.txt`.',\n",
       " 'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "  'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       " 'checksum': '29e9329ac2241937d55b852e8284e89b',\n",
       " 'file_name': 'glove-wiki-gigaword-300.gz',\n",
       " 'parts': 1}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.info(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(seq,word2idx,emb_dim,emb_file):\n",
    "    if os.path.exists(emb_file):\n",
    "        E=np.load(emb_file)\n",
    "    else:\n",
    "        vocab_size=len(word2idx)\n",
    "        E=np.zeros((vocab_size,emb_dim))\n",
    "        word_vectors=api.load(emb_model)\n",
    "        for word,idx in word2idx.items():\n",
    "            try:\n",
    "                E[idx]=word_vectors.word_vec(word)\n",
    "            except KeyError:\n",
    "                pass\n",
    "        np.save(emb_file,E)\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim=300\n",
    "data_dir=\"data\"\n",
    "emb_file=os.path.join(data_dir,\"E.npy\")\n",
    "emb_model=\"glove-wiki-gigaword-300\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "E=build_embedding_matrix(text_sequences_padding,word2idx,emb_dim,emb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E.shape == (len(word2idx),emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9013, 300)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining spam classifier with 1-dimensional convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamClassifierModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,vocab_sz,embed_sz,input_length,num_filters,kernel_sz,output_sz,run_mode,embedding_weights,**kwargs):\n",
    "        \n",
    "        super(SpamClassifierModel, self).__init__(**kwargs)\n",
    "        # in case of learning embeddings from scratch\n",
    "        if run_mode==\"scratch\":\n",
    "            self.embedding=tf.keras.layers.Embedding(vocab_sz\n",
    "            ,embed_sz,input_length=input_length,trainable=True)\n",
    "        # in case of transfer learning\n",
    "        elif run_mode==\"vectorizer\":\n",
    "            self.embedding=tf.keras.layers.Embedding(vocab_sz\n",
    "            ,embed_sz,input_length=input_length,weights=[embedding_weights],trainable=False)\n",
    "        # in case of fine-tuning parameters\n",
    "        else:\n",
    "            self.embedding=tf.keras.layers.Embedding(vocab_sz\n",
    "            ,embed_sz,input_length=input_length,weights=[embedding_weights],trainable=True)\n",
    "            \n",
    "        self.conv=tf.keras.layers.Conv1D(filters=num_filters,kernel_size=kernel_sz,activation=\"relu\")\n",
    "        self.dropout=tf.keras.layers.SpatialDropout1D(.2)\n",
    "        self.pool=tf.keras.layers.GlobalMaxPooling1D()\n",
    "        self.dense=tf.keras.layers.Dense(output_sz,activation=\"softmax\")\n",
    "\n",
    "    def call(self,X):\n",
    "        \n",
    "        X=self.embedding(X)\n",
    "        X=self.conv(X)\n",
    "        X=self.dropout(X)\n",
    "        X=self.pool(X)\n",
    "        X=self.dense(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining hyperparameters, option: learning embeddings from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_num_filters=256\n",
    "conv_kernel_size=3\n",
    "max_seq_len=text_sequences_padding.shape[1]\n",
    "model=SpamClassifierModel(vocab_size,emb_dim,max_seq_len,conv_num_filters,conv_kernel_size\n",
    "                         ,len(set(labels)),\"vectorizer\",E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(input_shape=(None,max_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"spam_classifier_model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      multiple                  2703900   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            multiple                  230656    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial multiple                  0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  514       \n",
      "=================================================================\n",
      "Total params: 2,935,070\n",
      "Trainable params: 231,170\n",
      "Non-trainable params: 2,703,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because number of documents described as spam is over 6 times higher than others, there is a need to\n",
    "# fix weights accurately by multiplying class weights with value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4827.,  747.], dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_labels.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.4618473"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val=cat_labels.sum(axis=0)[0] / cat_labels.sum(axis=0)[1] \n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=3\n",
    "class_weights={0:1,1:val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "29/29 [==============================] - 1s 29ms/step - loss: 0.0202 - accuracy: 0.9960 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/3\n",
      "29/29 [==============================] - 1s 28ms/step - loss: 0.0153 - accuracy: 0.9978 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "29/29 [==============================] - 1s 29ms/step - loss: 0.0109 - accuracy: 0.9984 - val_loss: 0.0051 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "r=model.fit(train_dataset,epochs=num_epochs,validation_data=val_dataset,class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.99595904, 0.9978448, 0.99838364]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels,predictions=[],[]\n",
    "for Xtest,Ytest in test_dataset:\n",
    "    Ytest_=model.predict_on_batch(Xtest)\n",
    "    ytest=np.argmax(Ytest,axis=1)\n",
    "    ytest_=np.argmax(Ytest_,axis=1)\n",
    "    labels.extend(ytest.tolist())\n",
    "    predictions.extend(ytest_.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.997\n",
      "confusion matrix\n",
      "[[1115    0]\n",
      " [   4  161]]\n"
     ]
    }
   ],
   "source": [
    "print(\"test accuracy: {:.3f}\".format(accuracy_score(labels,predictions)))\n",
    "print(\"confusion matrix\")\n",
    "print(confusion_matrix(labels,predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
